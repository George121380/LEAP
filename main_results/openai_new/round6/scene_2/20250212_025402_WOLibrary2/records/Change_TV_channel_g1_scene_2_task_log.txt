From agent.py
Reset goals: The sub-goals are: 
['Turn on the TV by touching the remote control.']
##############################
Goal representation from planning.py

#exp_behavior

behavior find_remote_control_2081_around_couch_192(remote_control:item):
    goal: not unknown(remote_control)
    body:
        assert is_remote_control(remote_control)
        bind couch_instance:item where:
            is_couch(couch_instance) and id[couch_instance]==192
        achieve close_char(char,couch_instance)
        if can_open(couch_instance):
            achieve_once open(couch_instance)
            exp(remote_control,couch_instance)
        else:
            exp(remote_control,couch_instance)
    eff:
        unknown[remote_control]=False
        close[remote_control,couch_instance]=True
        close[couch_instance,remote_control]=True
    

#exp_behavior_end

#goal_representation
 
behavior touch_remote_control(remote_control:item):
    body:
        achieve_once visited(remote_control)
        achieve_once touched(remote_control)

behavior __goal__():
    body:
        bind remote_control: item where:
            is_remote_control(remote_control)
        # Select the remote control
        touch_remote_control(remote_control)

#goal_representation_end

##############################
Inner TransformationError Debug
Error trying to process rule "behavior_definition":

'Function touched not found. Note that recursive function calls are not supported in the current version.'
##############################
Goal representation after debugging in planning.py
#exp_behavior

behavior find_remote_control_2081_around_couch_192(remote_control:item):
    goal: not unknown(remote_control)
    body:
        assert is_remote_control(remote_control)
        bind couch_instance:item where:
            is_couch(couch_instance) and id[couch_instance]==192
        achieve close_char(char,couch_instance)
        if can_open(couch_instance):
            achieve_once open(couch_instance)
            exp(remote_control,couch_instance)
        else:
            exp(remote_control,couch_instance)
    eff:
        unknown[remote_control]=False
        close[remote_control,couch_instance]=True
        close[couch_instance,remote_control]=True
    

#goal_representation
 
behavior touch_remote_control(remote_control:item):
    body:
        achieve_once visited(remote_control)
        touch(remote_control)

behavior __goal__():
    body:
        bind remote_control: item where:
            is_remote_control(remote_control)
        # Select the remote control
        touch_remote_control(remote_control)

#goal_representation_end

##############################
From agent.py->reset_goal
 
behavior touch_remote_control(remote_control:item):
    body:
        achieve_once visited(remote_control)
        touch(remote_control)

behavior __goal__():
    body:
        bind remote_control: item where:
            is_remote_control(remote_control)
        # Select the remote control
        touch_remote_control(remote_control)

##############################
From agent.py-> find a plan in act()
walk_executor(couch_192)exp(remote_control_2081, couch_192)walk_executor(remote_control_2081)obs(remote_control_2081, look around this place)touch_executor(remote_control_2081)
##############################
From agent.py
walk_executor(couch_192)
Robot find: pillow_195, vacuum_cleaner_2094, drawing_196, cat_2082, Robot is close to the couch_192. 
##############################
From agent.py
exp(remote_control_2081, couch_192)
Fail to find remote_control_2081 around couch_192. 
##############################
From agent.py-> find a plan in act()
walk_executor(tvstand_186)exp(remote_control_2081, tvstand_186)walk_executor(remote_control_2081)obs(remote_control_2081, look around this place)touch_executor(remote_control_2081)
##############################
From agent.py
walk_executor(tvstand_186)
Robot find: mat_201, cd_2100, Robot is close to the tvstand_186. 
##############################
From agent.py
exp(remote_control_2081, tvstand_186)
Fail to find remote_control_2081 around tvstand_186. 
##############################
From agent.py-> find a plan in act()
walk_executor(tvstand_135)exp(remote_control_2081, tvstand_135)walk_executor(remote_control_2081)obs(remote_control_2081, look around this place)touch_executor(remote_control_2081)
##############################
From agent.py
walk_executor(tvstand_135)
Robot find: drawing_118, remote_control_2081, cd_player_2084, dvd_player_2085, Robot is close to the tvstand_135. 
##############################
From agent.py
exp(remote_control_2081, tvstand_135)
Find remote_control_2081. 
##############################
From agent.py-> find a plan in act()
walk_executor(remote_control_2081)obs(remote_control_2081, look around this place)touch_executor(remote_control_2081)
##############################
From agent.py
walk_executor(remote_control_2081)
Robot find: Robot is close to the tvstand_135. Robot is close to the remote_control_2081. 
##############################
From agent.py
obs(remote_control_2081, look around this place)
Get this information:  remote_control_2081 is inside dining_room_1. remote_control_2081 is close to tvstand_135. remote_control_2081 is close to character_219. remote_control_2081 is on tvstand_135. remote_control_2081 is OFF.
##############################
From agent.py
touch_executor(remote_control_2081)
Robot find: Robot is close to the tvstand_135. Robot is close to the remote_control_2081. 
##############################
From agent.py -> evaluate_current_subgoal()
The evaluation result for current subgoal: Yes
The feedback instruction: None
##############################
Task Summary:
Task Goal:
Turn on the TV by touching the remote control.
Action History:
['walk_executor(couch_192)', 'walk_executor(tvstand_186)', 'walk_executor(tvstand_135)', 'walk_executor(remote_control_2081)', 'touch_executor(remote_control_2081)']
Time info:
Time consume: 45 seconds
Exp_helper query times: 0
Guidance query times: 0
library scale: 0
goal generate times: 1
goal correct times: 1
action_num: 5

Task complete rate:
1
Scene_id: 2
##############################
