From agent.py
Reset goals: The sub-goals are: 
['Turn on the TV by touching the remote control.']
##############################
Goal representation from planning.py

#exp_behavior

behavior find_remote_control_2124_around_couch_198(remote_control:item):
    goal: not unknown(remote_control)
    body:
        assert is_remote_control(remote_control)
        bind couch_instance:item where:
            is_couch(couch_instance) and id[couch_instance]==198
        achieve close_char(char,couch_instance)
        if can_open(couch_instance):
            achieve_once open(couch_instance)
            exp(remote_control,couch_instance)
        else:
            exp(remote_control,couch_instance)
    eff:
        unknown[remote_control]=False
        close[remote_control,couch_instance]=True
        close[couch_instance,remote_control]=True
    

#exp_behavior_end

#goal_representation
 
behavior turn_on_tv_with_remote(remote:item, tv:item):
    body:
        if not close_char(char, remote):
            walk_to(remote)
        
        if close_char(char, remote):
            touch(remote)

behavior __goal__():
    body:
        bind remote: item where:
            is_remote_control(remote)
        # Select the remote control

        bind tv: item where:
            is_television(tv)
        # Select the television

        turn_on_tv_with_remote(remote, tv)

#goal_representation_end

##############################
From agent.py->reset_goal
 
behavior turn_on_tv_with_remote(remote:item, tv:item):
    body:
        if not close_char(char, remote):
            walk_to(remote)
        
        if close_char(char, remote):
            touch(remote)

behavior __goal__():
    body:
        bind remote: item where:
            is_remote_control(remote)
        # Select the remote control

        bind tv: item where:
            is_television(tv)
        # Select the television

        turn_on_tv_with_remote(remote, tv)

##############################
From agent.py-> find a plan in act()
walk_executor(couch_198)exp(remote_control_2124, couch_198)walk_executor(remote_control_2124)touch_executor(remote_control_2124)
##############################
From agent.py
walk_executor(couch_198)
Robot find: chair_2003, instrument_guitar_2028, rag_2052, mat_203, phone_2016, clothes_jacket_2119, Robot is close to the couch_198. 
##############################
From agent.py
exp(remote_control_2124, couch_198)
Fail to find remote_control_2124 around couch_198. 
##############################
From agent.py-> find a plan in act()
walk_executor(television_281)exp(remote_control_2124, television_281)walk_executor(remote_control_2124)touch_executor(remote_control_2124)
##############################
From agent.py
walk_executor(television_281)
Robot find: drawing_288, cd_2145, mat_284, headset_2131, cd_player_2129, remote_control_2124, dvd_player_2130, Robot is close to the television_281. Robot is close to the tvstand_273. 
##############################
From agent.py
exp(remote_control_2124, television_281)
Find remote_control_2124. 
##############################
From agent.py-> find a plan in act()
walk_executor(remote_control_2124)touch_executor(remote_control_2124)
##############################
From agent.py
walk_executor(remote_control_2124)
Robot find: Robot is close to the tvstand_273. Robot is close to the remote_control_2124. 
##############################
From agent.py
touch_executor(remote_control_2124)
Robot find: Robot is close to the tvstand_273. Robot is close to the remote_control_2124. 
##############################
From agent.py -> evaluate_current_subgoal()
The evaluation result for current subgoal: Yes
The feedback instruction: None
##############################
Task Summary:
Task Goal:
Turn on the TV by touching the remote control.
Action History:
['walk_executor(couch_198)', 'walk_executor(television_281)', 'walk_executor(remote_control_2124)', 'touch_executor(remote_control_2124)']
Time info:
Time consume: 48 seconds
Exp_helper query times: 0
Guidance query times: 0
library scale: 59
goal generate times: 1
goal correct times: 0
action_num: 4

Task complete rate:
1
Scene_id: 1
##############################
